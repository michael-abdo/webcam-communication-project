#!/usr/bin/env python3
"""
Test Cognitive Expression Detection on ALL Major Face Datasets

This script tests cognitive overload detection on ALL major face datasets:
1. Faces in Event Streams (FES) - 689 minutes, 1.6M+ faces
2. MobiFace Dataset - 80 mobile videos, 95K+ bounding boxes  
3. Selfies and Videos Dataset (Kaggle) - 4,200+ video sets
4. Web Camera Face Liveness Detection (Kaggle) - 30,000+ videos
5. Previous validation datasets

Tests brow furrow, eye strain, mouth tension, and cognitive stress indicators.
"""

import os
import sys
import json
import numpy as np
import cv2
from typing import Dict, List, Tuple, Any
import matplotlib.pyplot as plt
from datetime import datetime
import glob

sys.path.append('../processing')
from landmark_processor import LandmarkProcessor
from landmark_mapping import CognitiveLandmarkMapper

class MajorDatasetCognitiveTester:
    """Test cognitive expression detection on all major face datasets."""
    
    def __init__(self, results_dir: str = "./major_dataset_cognitive_results"):
        """Initialize comprehensive tester for all major datasets."""
        self.results_dir = results_dir
        os.makedirs(self.results_dir, exist_ok=True)
        
        # Initialize cognitive mapper
        self.cognitive_mapper = CognitiveLandmarkMapper()
        
        # Cognitive metrics to test
        self.cognitive_metrics = [
            'brow_furrow_distance',
            'left_eye_openness', 
            'right_eye_openness',
            'avg_eye_openness',
            'mouth_compression',
            'cognitive_stress_score'
        ]
        
        # ALL major face datasets configuration
        self.major_datasets = [
            {
                'name': 'major_face_datasets/faces_event_streams',
                'dataset_type': 'FES',
                'expected_state': 'event_stream_faces',
                'description': 'Faces in Event Streams - 689 minutes with 1.6M+ faces at 30Hz',
                'characteristics': 'high_volume_real_time',
                'expected_metrics': {
                    'brow_furrow_distance': (70, 130),
                    'avg_eye_openness': (0.08, 0.4),
                    'mouth_compression': (0.0, 0.3),
                    'cognitive_stress_score': (0.1, 1.0)
                }
            },
            {
                'name': 'major_face_datasets/mobiface',
                'dataset_type': 'MobiFace',
                'expected_state': 'mobile_livestream_faces',
                'description': 'MobiFace - 80 mobile videos from 70 users, 95K+ bounding boxes',
                'characteristics': 'unconstrained_mobile',
                'expected_metrics': {
                    'brow_furrow_distance': (75, 125),
                    'avg_eye_openness': (0.1, 0.35),
                    'mouth_compression': (0.0, 0.25),
                    'cognitive_stress_score': (0.2, 0.9)
                }
            },
            {
                'name': 'major_face_datasets/selfies_videos_kaggle',
                'dataset_type': 'Kaggle_Selfies',
                'expected_state': 'selfie_videos',
                'description': 'Kaggle Selfies - 4,200+ video sets for face recognition',
                'characteristics': 'high_volume_selfies',
                'expected_metrics': {
                    'brow_furrow_distance': (80, 120),
                    'avg_eye_openness': (0.1, 0.3),
                    'mouth_compression': (0.0, 0.2),
                    'cognitive_stress_score': (0.3, 0.8)
                }
            },
            {
                'name': 'major_face_datasets/webcam_liveness',
                'dataset_type': 'Kaggle_Liveness',
                'expected_state': 'liveness_detection',
                'description': 'Kaggle Liveness - 30,000+ videos for anti-spoofing',
                'characteristics': 'webcam_security',
                'expected_metrics': {
                    'brow_furrow_distance': (70, 135),
                    'avg_eye_openness': (0.05, 0.45),
                    'mouth_compression': (0.0, 0.35),
                    'cognitive_stress_score': (0.0, 1.0)
                }
            },
            # Include existing validation datasets for comparison
            {
                'name': 'live_face_datasets/selfies_videos_kaggle',
                'dataset_type': 'Validation_Real',
                'expected_state': 'validation_real_faces',
                'description': 'Validation Real Faces - Previously tested Kaggle selfies',
                'characteristics': 'validation_baseline',
                'expected_metrics': {
                    'brow_furrow_distance': (80, 120),
                    'avg_eye_openness': (0.1, 0.3),
                    'mouth_compression': (0.0, 0.2),
                    'cognitive_stress_score': (0.3, 0.8)
                }
            },
            {
                'name': 'real_face_datasets/validation_dataset',
                'dataset_type': 'Validation_Synthetic',
                'expected_state': 'validation_synthetic',
                'description': 'Validation Synthetic Faces - High-quality synthetic expressions',
                'characteristics': 'synthetic_controlled',
                'expected_metrics': {
                    'brow_furrow_distance': (90, 100),
                    'avg_eye_openness': (0.14, 0.16),
                    'mouth_compression': (0.0, 0.01),
                    'cognitive_stress_score': (0.6, 0.65)
                }
            }
        ]
    
    def find_videos_in_dataset(self, dataset_path: str) -> List[str]:
        """Find all video files in a dataset directory."""
        if not os.path.exists(dataset_path):
            return []
        
        video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.m4v', '.flv', '.webm']
        video_files = []
        
        # Search recursively for video files
        for root, dirs, files in os.walk(dataset_path):
            for file in files:
                if any(file.lower().endswith(ext) for ext in video_extensions):
                    video_files.append(os.path.join(root, file))
        
        return sorted(video_files)
    
    def analyze_video_cognitive_metrics(self, video_path: str) -> Dict[str, Any]:
        """Analyze cognitive metrics for a single video with enhanced error handling."""
        print(f"\\nAnalyzing cognitive expressions: {os.path.basename(video_path)}")
        
        try:
            # Create landmark processor for this specific video
            landmark_processor = LandmarkProcessor(video_path)
            
            # Process video with landmark detection
            landmarks_data = landmark_processor.process_video()
            
            if not landmarks_data or 'landmarks_data' not in landmarks_data:
                return {
                    'success': False,
                    'error': 'No landmark data extracted',
                    'video_path': video_path,
                    'video_size_mb': self._get_file_size_mb(video_path)
                }
            
            frames_data = landmarks_data['landmarks_data']
            
            if not frames_data:
                return {
                    'success': False,
                    'error': 'No frames with face detection',
                    'video_path': video_path,
                    'video_size_mb': self._get_file_size_mb(video_path)
                }
            
            # Calculate cognitive metrics for each frame
            cognitive_results = []
            
            for frame_data in frames_data:
                if frame_data.get('face_detected', False):
                    landmarks = frame_data['landmarks']
                    
                    # Calculate all cognitive metrics
                    metrics = {}
                    
                    # Brow furrow (concentration/stress indicator)
                    metrics['brow_furrow_distance'] = self._calculate_brow_furrow_distance(landmarks)
                    
                    # Eye openness (fatigue/alertness indicator)
                    metrics['left_eye_openness'] = self._calculate_eye_openness(landmarks, 'left')
                    metrics['right_eye_openness'] = self._calculate_eye_openness(landmarks, 'right')
                    metrics['avg_eye_openness'] = (metrics['left_eye_openness'] + metrics['right_eye_openness']) / 2
                    
                    # Mouth tension (stress/anxiety indicator)
                    metrics['mouth_compression'] = self._calculate_mouth_compression(landmarks)
                    
                    # Overall cognitive stress score
                    metrics['cognitive_stress_score'] = self._calculate_cognitive_stress_score(
                        metrics['brow_furrow_distance'],
                        metrics['avg_eye_openness'],
                        metrics['mouth_compression']
                    )
                    
                    cognitive_results.append(metrics)
            
            if not cognitive_results:
                return {
                    'success': False,
                    'error': 'No cognitive metrics calculated',
                    'video_path': video_path,
                    'video_size_mb': self._get_file_size_mb(video_path)
                }
            
            # Calculate statistics across all frames
            stats = self._calculate_cognitive_statistics(cognitive_results)
            
            # Analyze cognitive patterns
            patterns = self._analyze_cognitive_patterns(cognitive_results)
            
            # Video metadata
            metadata = landmarks_data.get('metadata', {})
            
            return {
                'success': True,
                'video_path': video_path,
                'video_size_mb': self._get_file_size_mb(video_path),
                'frames_analyzed': len(cognitive_results),
                'cognitive_statistics': stats,
                'cognitive_patterns': patterns,
                'raw_metrics': cognitive_results[:5],  # Sample of raw data
                'processing_info': {
                    'total_frames': metadata.get('total_frames_processed', 0),
                    'face_detection_rate': len(cognitive_results) / max(1, metadata.get('total_frames_processed', 1)),
                    'video_metadata': metadata.get('video_metadata', {})
                }
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': f"Processing error: {str(e)}",
                'video_path': video_path,
                'video_size_mb': self._get_file_size_mb(video_path)
            }
    
    def _get_file_size_mb(self, file_path: str) -> float:
        """Get file size in MB."""
        try:
            return os.path.getsize(file_path) / (1024 * 1024)
        except:
            return 0.0
    
    def _calculate_brow_furrow_distance(self, landmarks: List[Tuple[float, float]]) -> float:
        """Calculate brow furrow distance (concentration indicator)."""
        try:
            left_brow = self.cognitive_mapper.left_eyebrow_landmarks
            right_brow = self.cognitive_mapper.right_eyebrow_landmarks
            
            left_inner = landmarks[left_brow[0]]
            right_inner = landmarks[right_brow[0]]
            
            distance = np.sqrt((left_inner[0] - right_inner[0])**2 + (left_inner[1] - right_inner[1])**2)
            return distance * 1000
            
        except (IndexError, KeyError):
            return 95.0
    
    def _calculate_eye_openness(self, landmarks: List[Tuple[float, float]], eye: str) -> float:
        """Calculate eye openness ratio (fatigue indicator)."""
        try:
            if eye == 'left':
                eye_landmarks = self.cognitive_mapper.left_eye_landmarks
            else:
                eye_landmarks = self.cognitive_mapper.right_eye_landmarks
            
            upper_lid = eye_landmarks['upper_lid'][:4]
            lower_lid = eye_landmarks['lower_lid'][:4]
            
            vertical_distances = []
            for i in range(min(len(upper_lid), len(lower_lid))):
                upper_point = landmarks[upper_lid[i]]
                lower_point = landmarks[lower_lid[i]]
                vertical_dist = abs(upper_point[1] - lower_point[1])
                vertical_distances.append(vertical_dist)
            
            avg_vertical = np.mean(vertical_distances)
            
            inner_corner = landmarks[eye_landmarks['inner_corner'][0]]
            outer_corner = landmarks[eye_landmarks['outer_corner'][0]]
            horizontal_dist = abs(outer_corner[0] - inner_corner[0])
            
            return avg_vertical / max(horizontal_dist, 0.001)
            
        except (IndexError, KeyError):
            return 0.15
    
    def _calculate_mouth_compression(self, landmarks: List[Tuple[float, float]]) -> float:
        """Calculate mouth compression ratio (stress indicator)."""
        try:
            mouth_landmarks = self.cognitive_mapper.mouth_landmarks
            
            left_corner = landmarks[mouth_landmarks['corners'][0]]
            right_corner = landmarks[mouth_landmarks['corners'][1]]
            
            top_center = landmarks[mouth_landmarks['center_top'][0]]
            bottom_center = landmarks[mouth_landmarks['center_bottom'][0]]
            
            horizontal_dist = abs(right_corner[0] - left_corner[0])
            vertical_dist = abs(top_center[1] - bottom_center[1])
            
            compression_ratio = vertical_dist / max(horizontal_dist, 0.001)
            return max(0, 0.5 - compression_ratio)
            
        except (IndexError, KeyError):
            return 0.1
    
    def _calculate_cognitive_stress_score(self, brow_furrow: float, eye_openness: float, mouth_compression: float) -> float:
        """Calculate overall cognitive stress score (0-1 scale)."""
        try:
            brow_stress = max(0, min(1, (100 - brow_furrow) / 20))
            eye_stress = max(0, min(1, (0.25 - eye_openness) / 0.15))
            mouth_stress = max(0, min(1, mouth_compression / 0.3))
            
            stress_score = (0.5 * brow_stress + 0.3 * eye_stress + 0.2 * mouth_stress)
            return max(0, min(1, stress_score))
            
        except:
            return 0.5
    
    def _calculate_cognitive_statistics(self, cognitive_data: List[Dict]) -> Dict[str, Dict]:
        """Calculate statistics for cognitive metrics."""
        stats = {}
        
        for metric in self.cognitive_metrics:
            values = [frame[metric] for frame in cognitive_data if metric in frame]
            
            if values:
                stats[metric] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'median': np.median(values),
                    'count': len(values)
                }
            else:
                stats[metric] = {
                    'mean': 0, 'std': 0, 'min': 0, 'max': 0, 'median': 0, 'count': 0
                }
        
        return stats
    
    def _analyze_cognitive_patterns(self, cognitive_data: List[Dict]) -> Dict[str, Any]:
        """Analyze patterns in cognitive metrics over time."""
        patterns = {}
        
        # Stress level classification
        stress_scores = [frame['cognitive_stress_score'] for frame in cognitive_data]
        avg_stress = np.mean(stress_scores)
        
        if avg_stress < 0.4:
            stress_level = 'LOW'
        elif avg_stress < 0.7:
            stress_level = 'MODERATE'
        else:
            stress_level = 'HIGH'
        
        patterns['stress_level'] = stress_level
        patterns['stress_score_avg'] = avg_stress
        
        # Eye strain analysis
        eye_openness = [frame['avg_eye_openness'] for frame in cognitive_data]
        avg_eye_openness = np.mean(eye_openness)
        
        if avg_eye_openness < 0.1:
            eye_state = 'VERY_TIRED'
        elif avg_eye_openness < 0.15:
            eye_state = 'TIRED'
        elif avg_eye_openness < 0.25:
            eye_state = 'NORMAL'
        else:
            eye_state = 'ALERT'
        
        patterns['eye_state'] = eye_state
        patterns['eye_openness_avg'] = avg_eye_openness
        
        # Concentration analysis
        brow_distances = [frame['brow_furrow_distance'] for frame in cognitive_data]
        avg_brow_distance = np.mean(brow_distances)
        
        if avg_brow_distance < 85:
            concentration_level = 'HIGH_CONCENTRATION'
        elif avg_brow_distance < 95:
            concentration_level = 'MODERATE_CONCENTRATION'
        else:
            concentration_level = 'RELAXED'
        
        patterns['concentration_level'] = concentration_level
        patterns['brow_furrow_avg'] = avg_brow_distance
        
        return patterns
    
    def test_dataset_cognitive_expressions(self, dataset_config: Dict) -> Dict[str, Any]:
        """Test cognitive expression detection on a major dataset."""
        print(f"\\n{'='*80}")
        print(f"TESTING MAJOR DATASET: {dataset_config['dataset_type']}")
        print(f"Dataset: {dataset_config['name']}")
        print(f"Description: {dataset_config['description']}")
        print(f"Characteristics: {dataset_config['characteristics']}")
        print(f"{'='*80}")
        
        dataset_path = dataset_config['name']
        
        if not os.path.exists(dataset_path):
            print(f"‚ö†Ô∏è  Dataset not found: {dataset_path}")
            return {
                'success': False,
                'error': f"Dataset path not found: {dataset_path}",
                'dataset': dataset_config['name'],
                'dataset_type': dataset_config['dataset_type']
            }
        
        # Find video files
        video_files = self.find_videos_in_dataset(dataset_path)
        
        if not video_files:
            print(f"‚ö†Ô∏è  No video files found in {dataset_path}")
            return {
                'success': False,
                'error': f"No video files found in {dataset_path}",
                'dataset': dataset_config['name'],
                'dataset_type': dataset_config['dataset_type']
            }
        
        print(f"Found {len(video_files)} videos to analyze")
        
        # Test sample of videos (limit for performance)
        max_videos = min(15, len(video_files))  # Test up to 15 videos per major dataset
        test_videos = video_files[:max_videos]
        video_results = []
        
        total_size_mb = sum(self._get_file_size_mb(v) for v in test_videos)
        print(f"Testing {len(test_videos)} videos ({total_size_mb:.1f} MB total)")
        
        for i, video_path in enumerate(test_videos, 1):
            print(f"\\nProgress: {i}/{len(test_videos)}")
            result = self.analyze_video_cognitive_metrics(video_path)
            video_results.append(result)
            
            if result['success']:
                patterns = result['cognitive_patterns']
                print(f"   ‚úÖ {os.path.basename(video_path)} ({result['video_size_mb']:.1f}MB)")
                print(f"      Stress: {patterns['stress_level']}, Eye: {patterns['eye_state']}, Focus: {patterns['concentration_level']}")
            else:
                print(f"   ‚ùå {os.path.basename(video_path)}: {result['error']}")
        
        # Aggregate results
        successful_results = [r for r in video_results if r['success']]
        
        if not successful_results:
            return {
                'success': False,
                'error': 'No videos were successfully analyzed',
                'dataset': dataset_config['name'],
                'dataset_type': dataset_config['dataset_type'],
                'video_results': video_results
            }
        
        # Calculate dataset-wide cognitive metrics
        dataset_cognitive_summary = self._summarize_dataset_cognitive_metrics(
            successful_results, dataset_config
        )
        
        return {
            'success': True,
            'dataset': dataset_config['name'],
            'dataset_type': dataset_config['dataset_type'],
            'expected_state': dataset_config['expected_state'],
            'characteristics': dataset_config['characteristics'],
            'videos_analyzed': len(successful_results),
            'videos_total': len(test_videos),
            'total_videos_found': len(video_files),
            'cognitive_summary': dataset_cognitive_summary,
            'video_results': video_results,
            'validation_results': self._validate_against_expectations(
                dataset_cognitive_summary, dataset_config['expected_metrics']
            )
        }
    
    def _summarize_dataset_cognitive_metrics(self, results: List[Dict], dataset_config: Dict) -> Dict[str, Any]:
        """Summarize cognitive metrics across all videos in dataset."""
        summary = {}
        
        # Collect all cognitive statistics
        all_stats = {}
        for metric in self.cognitive_metrics:
            all_stats[metric] = {'means': [], 'stds': []}
        
        for result in results:
            stats = result['cognitive_statistics']
            for metric in self.cognitive_metrics:
                if metric in stats:
                    all_stats[metric]['means'].append(stats[metric]['mean'])
                    all_stats[metric]['stds'].append(stats[metric]['std'])
        
        # Calculate dataset-wide statistics
        for metric in self.cognitive_metrics:
            if all_stats[metric]['means']:
                summary[metric] = {
                    'dataset_mean': np.mean(all_stats[metric]['means']),
                    'dataset_std': np.mean(all_stats[metric]['stds']),
                    'video_count': len(all_stats[metric]['means']),
                    'range': (
                        np.min(all_stats[metric]['means']),
                        np.max(all_stats[metric]['means'])
                    )
                }
        
        # Aggregate cognitive patterns
        all_patterns = [r['cognitive_patterns'] for r in results]
        
        stress_levels = [p['stress_level'] for p in all_patterns]
        eye_states = [p['eye_state'] for p in all_patterns]
        concentration_levels = [p['concentration_level'] for p in all_patterns]
        
        summary['pattern_distribution'] = {
            'stress_levels': {level: stress_levels.count(level) for level in set(stress_levels)},
            'eye_states': {state: eye_states.count(state) for state in set(eye_states)},
            'concentration_levels': {level: concentration_levels.count(level) for level in set(concentration_levels)}
        }
        
        # Overall dataset characterization
        avg_stress_scores = [p['stress_score_avg'] for p in all_patterns]
        summary['dataset_characterization'] = {
            'primary_stress_level': max(summary['pattern_distribution']['stress_levels'], 
                                      key=summary['pattern_distribution']['stress_levels'].get),
            'primary_eye_state': max(summary['pattern_distribution']['eye_states'],
                                   key=summary['pattern_distribution']['eye_states'].get),
            'primary_concentration': max(summary['pattern_distribution']['concentration_levels'],
                                       key=summary['pattern_distribution']['concentration_levels'].get),
            'avg_stress_score': np.mean(avg_stress_scores),
            'stress_score_variability': np.std(avg_stress_scores),
            'dataset_type': dataset_config['dataset_type'],
            'characteristics': dataset_config['characteristics']
        }
        
        return summary
    
    def _validate_against_expectations(self, summary: Dict, expected_metrics: Dict) -> Dict[str, Any]:
        """Validate cognitive metrics against expected ranges."""
        validation = {}
        
        for metric, expected_range in expected_metrics.items():
            if metric in summary:
                actual_value = summary[metric]['dataset_mean']
                min_expected, max_expected = expected_range
                
                in_range = min_expected <= actual_value <= max_expected
                
                validation[metric] = {
                    'expected_range': expected_range,
                    'actual_value': actual_value,
                    'in_expected_range': in_range,
                    'status': 'PASS' if in_range else 'FAIL'
                }
            else:
                validation[metric] = {
                    'expected_range': expected_range,
                    'actual_value': None,
                    'in_expected_range': False,
                    'status': 'NO_DATA'
                }
        
        # Overall validation status
        passed_metrics = sum(1 for v in validation.values() if v['status'] == 'PASS')
        total_metrics = len(validation)
        
        validation['overall'] = {
            'passed_metrics': passed_metrics,
            'total_metrics': total_metrics,
            'pass_rate': passed_metrics / total_metrics if total_metrics > 0 else 0,
            'status': 'PASS' if passed_metrics >= total_metrics * 0.6 else 'FAIL'  # 60% threshold for major datasets
        }
        
        return validation
    
    def run_comprehensive_major_dataset_tests(self) -> Dict[str, Any]:
        """Run comprehensive cognitive expression tests on ALL major datasets."""
        print("=" * 100)
        print("COMPREHENSIVE COGNITIVE EXPRESSION TESTING - ALL MAJOR FACE DATASETS")
        print("Testing brow furrow, eye strain, mouth tension, and stress on:")
        print("‚Ä¢ Faces in Event Streams (FES) ‚Ä¢ MobiFace ‚Ä¢ Kaggle Selfies ‚Ä¢ Kaggle Liveness")
        print("=" * 100)
        
        all_results = {}
        
        for dataset_config in self.major_datasets:
            result = self.test_dataset_cognitive_expressions(dataset_config)
            all_results[dataset_config['dataset_type']] = result
        
        # Generate comprehensive report
        report = self._generate_major_dataset_report(all_results)
        
        # Save results
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = os.path.join(self.results_dir, f'major_datasets_cognitive_results_{timestamp}.json')
        
        with open(results_file, 'w') as f:
            json.dump({
                'timestamp': timestamp,
                'test_summary': report,
                'detailed_results': all_results
            }, f, indent=2, default=str)
        
        print(f"\\nüìä Comprehensive results saved to: {results_file}")
        
        return {
            'test_summary': report,
            'detailed_results': all_results,
            'results_file': results_file
        }
    
    def _generate_major_dataset_report(self, all_results: Dict) -> Dict[str, Any]:
        """Generate comprehensive report for all major datasets."""
        report = {
            'executive_summary': {},
            'dataset_comparison': {},
            'cognitive_metrics_validation': {},
            'performance_analysis': {},
            'recommendations': []
        }
        
        successful_datasets = {k: v for k, v in all_results.items() if v.get('success', False)}
        
        # Executive Summary
        total_datasets = len(all_results)
        successful_count = len(successful_datasets)
        total_videos = sum(r.get('videos_analyzed', 0) for r in successful_datasets.values())
        total_videos_found = sum(r.get('total_videos_found', 0) for r in successful_datasets.values())
        
        report['executive_summary'] = {
            'major_datasets_tested': total_datasets,
            'successful_datasets': successful_count,
            'success_rate': successful_count / total_datasets if total_datasets > 0 else 0,
            'total_videos_analyzed': total_videos,
            'total_videos_available': total_videos_found,
            'cognitive_detection_working': successful_count > 0
        }
        
        # Dataset Comparison
        for dataset_type, result in successful_datasets.items():
            if 'cognitive_summary' in result:
                summary = result['cognitive_summary']
                char = summary.get('dataset_characterization', {})
                
                report['dataset_comparison'][dataset_type] = {
                    'dataset_type': result.get('dataset_type', dataset_type),
                    'characteristics': result.get('characteristics', 'unknown'),
                    'videos_analyzed': result.get('videos_analyzed', 0),
                    'primary_stress_level': char.get('primary_stress_level', 'UNKNOWN'),
                    'primary_eye_state': char.get('primary_eye_state', 'UNKNOWN'),
                    'concentration_level': char.get('primary_concentration', 'UNKNOWN'),
                    'avg_stress_score': char.get('avg_stress_score', 0),
                    'stress_variability': char.get('stress_score_variability', 0)
                }
        
        # Cognitive Metrics Validation
        all_validations = []
        for dataset_type, result in successful_datasets.items():
            if 'validation_results' in result:
                validation = result['validation_results']
                all_validations.append({
                    'dataset_type': dataset_type,
                    'pass_rate': validation['overall']['pass_rate'],
                    'status': validation['overall']['status']
                })
        
        if all_validations:
            avg_pass_rate = np.mean([v['pass_rate'] for v in all_validations])
            passing_datasets = sum(1 for v in all_validations if v['status'] == 'PASS')
            
            report['cognitive_metrics_validation'] = {
                'average_pass_rate': avg_pass_rate,
                'datasets_passing': passing_datasets,
                'datasets_tested': len(all_validations),
                'overall_validation_status': 'PASS' if avg_pass_rate >= 0.6 else 'NEEDS_IMPROVEMENT'
            }
        
        # Performance Analysis
        dataset_performance = {}
        for dataset_type, result in successful_datasets.items():
            if result.get('success', False):
                dataset_performance[dataset_type] = {
                    'videos_success_rate': result.get('videos_analyzed', 0) / max(1, result.get('videos_total', 1)),
                    'total_videos_available': result.get('total_videos_found', 0),
                    'characteristics': result.get('characteristics', 'unknown')
                }
        
        report['performance_analysis'] = dataset_performance
        
        # Recommendations
        if report['executive_summary']['cognitive_detection_working']:
            if report['cognitive_metrics_validation'].get('overall_validation_status') == 'PASS':
                report['recommendations'].extend([
                    "‚úÖ Cognitive expression detection validated across major datasets",
                    "‚úÖ System performs well on real-world, mobile, and webcam data",
                    "‚úÖ Ready for production deployment with diverse face inputs",
                    "üöÄ Proceed with large-scale user testing"
                ])
            else:
                report['recommendations'].extend([
                    "‚ö†Ô∏è Cognitive metrics show variations across dataset types",
                    "üìä Consider dataset-specific calibration",
                    "üîß Fine-tune thresholds for different input sources"
                ])
        else:
            report['recommendations'].extend([
                "‚ùå Major dataset testing revealed issues",
                "üîç Investigate dataset-specific failures",
                "‚ö° Ensure all major datasets are properly downloaded"
            ])
        
        return report

def main():
    """Main execution function."""
    print("MAJOR FACE DATASETS COGNITIVE EXPRESSION TESTING")
    print("Testing cognitive overload detection on ALL major datasets")
    print("=" * 100)
    
    tester = MajorDatasetCognitiveTester()
    results = tester.run_comprehensive_major_dataset_tests()
    
    # Print summary report
    print("\\n" + "=" * 100)
    print("MAJOR DATASETS COGNITIVE TESTING FINAL REPORT")
    print("=" * 100)
    
    summary = results['test_summary']
    exec_summary = summary['executive_summary']
    
    print(f"\\nüìä EXECUTIVE SUMMARY:")
    print(f"   Major Datasets Tested: {exec_summary['major_datasets_tested']}")
    print(f"   Successful Analysis: {exec_summary['successful_datasets']}")
    print(f"   Videos Analyzed: {exec_summary['total_videos_analyzed']}")
    print(f"   Videos Available: {exec_summary['total_videos_available']}")
    print(f"   Cognitive Detection Working: {'‚úÖ YES' if exec_summary['cognitive_detection_working'] else '‚ùå NO'}")
    
    if 'cognitive_metrics_validation' in summary:
        validation = summary['cognitive_metrics_validation']
        print(f"\\nüéØ VALIDATION RESULTS:")
        print(f"   Average Pass Rate: {validation['average_pass_rate']:.1%}")
        print(f"   Overall Status: {validation['overall_validation_status']}")
    
    print(f"\\nüîç DATASET COMPARISON:")
    for dataset_type, analysis in summary.get('dataset_comparison', {}).items():
        print(f"   {dataset_type} ({analysis['characteristics']}):")
        print(f"     - Videos: {analysis['videos_analyzed']}")
        print(f"     - Stress: {analysis['primary_stress_level']}")
        print(f"     - Eyes: {analysis['primary_eye_state']}")
        print(f"     - Focus: {analysis['concentration_level']}")
        print(f"     - Avg Stress Score: {analysis['avg_stress_score']:.3f}")
    
    print(f"\\nüí° RECOMMENDATIONS:")
    for rec in summary['recommendations']:
        print(f"   {rec}")
    
    print(f"\\nüìÅ Detailed results: {results['results_file']}")

if __name__ == "__main__":
    main()